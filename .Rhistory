# Convert 'close_to_beach' to a factor (if it's not already)
testPP$close_to_beach <- as.factor(testPP$close_to_beach)
# Create a dummy variable for 'close_to_beach'
# Assuming "Yes" = 1, "No" = 0
testPP$close_to_beachYes <- ifelse(testPP$close_to_beach == "Yes", 1, 0)
# Optionally, remove the original 'close_to_beach' column
testPP$close_to_beach <- NULL
# Create dummy variables for each level of 'area'
area_dummies <- model.matrix(~ area - 1, data = testPP)
# Rename the columns to the desired names
colnames(area_dummies) <- c("areaA", "areaB", "areaC", "areaS")
# Add the dummy variables back to the original data frame
testPP <- cbind(testPP, area_dummies)
area_num <- NULL
# Scale numerical variables (standardization)
numeric_cols <- c("dist", "rating", "size", "rooms", "max_guests", "bathrooms")
testPP[numeric_cols] <- scale(testPP[numeric_cols])
scaled_test_data <- testPP
predicted_prices_test <- predict(fLM, newdata = scaled_test_data)
scaled_test_data$predicted_price_test <- predicted_prices_test
#write.table(scaled_test_data$predicted_price_test, file = "Round3PredFINAL.txt", sep = "\t", row.names = FALSE, quote = TRUE)
predValues <- read.csv("Round3PredFINAL.txt", sep = ",")
PredTest <- cbind(predValues, test)
View(PredTest)
setwd("F:\\Stellenbosch Uni\\Semester2\\DSProject")
dat <- read.csv("F:\\Stellenbosch Uni\\Semester2\\DSProject\\project_airbnb_data.txt", sep = ";",
header = T, stringsAsFactors = T)
test <- read.csv("F:\\Stellenbosch Uni\\Semester2\\DSProject\\project_airbnb_data_test.txt", sep = ";",
header = T, stringsAsFactors = T)
library(leaps)
library(nortest)
library(dplyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(Metrics)
library(car)
# Ons install extra vir die Metrics en car package.
# Metrics is om die RMSE te calculate
# car is om die autocorrelation te calculate
datPP <- dat
# Convert 'season' and 'close_to_beach' into factors if not already
datPP$season <- as.factor(datPP$season)
datPP$close_to_beach <- as.factor(datPP$close_to_beach)
# Create dummy variables for each season level
season_dummies <- model.matrix(~ season - 1, data = datPP)
# Add the dummy variables back to the original data frame
datPP <- cbind(datPP, season_dummies)
datPP$season <- NULL
# Convert 'close_to_beach' to a factor (if it's not already)
datPP$close_to_beach <- as.factor(datPP$close_to_beach)
# Create a dummy variable for 'close_to_beach'
# Assuming "Yes" = 1, "No" = 0
datPP$close_to_beachYes <- ifelse(datPP$close_to_beach == "Yes", 1, 0)
# Optionally, remove the original 'close_to_beach' column
datPP$close_to_beach <- NULL
# Create dummy variables for each level of 'area'
area_dummies <- model.matrix(~ area - 1, data = datPP)
# Rename the columns to the desired names
colnames(area_dummies) <- c("areaA", "areaB", "areaC", "areaS")
# Add the dummy variables back to the original data frame
datPP <- cbind(datPP, area_dummies)
datPP$area_num <- NULL
# Scale numerical variables (standardization)
numeric_cols <- c("dist", "rating", "size", "rooms", "max_guests", "bathrooms")
datPP[numeric_cols] <- scale(datPP[numeric_cols])
# Calculate Z-scores for price and filter out extreme outliers horizontal
z_scores <- scale(datPP$price)
datPP <- datPP[abs(z_scores) < 3, ]  # Keep only data within 3 standard deviations
# Onthou om te se hoeveel observations remove is en hoeveel nuwe vars create is.
scaled_data <- datPP
set.seed(1234)
sample <- sample(c(TRUE, FALSE), nrow(scaled_data), replace=TRUE, prob=c(0.8,0.2))
train  <- scaled_data[sample, ]
valid   <- scaled_data[!sample, ]
baseModel <- lm(price ~ 1, train)
fwd <- step((baseModel), direction= "forward", list(lower=~1, upper=~dist+seasonAutumn+seasonSpring+                                                    rating+seasonSummer+areaA+areaB+areaC+areaS+bathrooms+close_to_beachYes+
seasonWinter+max_guests+size+rooms), data = train
, trace = F)
model_All <- lm(price ~ areaA + areaB + areaC + areaS + seasonWinter + seasonAutumn + close_to_beachYes +
seasonSpring + max_guests + rating + size + bathrooms + dist, train)
bwd <- step((model_All), direction= "backward", list(lower=~1, upper=~.),  data = train
, trace = F)
# Het hier vir die bwd model se vars gebruik omdat dit n laer AIC het.
evaluate_polynomial_model <- function(degree, train_data) {
model <- lm(price ~ areaA + areaB + areaC + seasonWinter + seasonAutumn
+ close_to_beachYes + seasonSpring + poly(max_guests, 10)
+ poly(rating, d) + poly(size, d)
+ poly(bathrooms, 3) + poly(dist, d), train)
predictions <- predict(model, newdata = train_data)
rmse <- sqrt(mean((predictions - train_data$price)^2))
return(rmse)
}
# Initialize a vector to store RMSE values
degree_range <- 1:25
rmse_values <- numeric(length(degree_range))
# Evaluate models for different polynomial degrees
for (d in degree_range) {
rmse_values[d] <- evaluate_polynomial_model(d, train)
}
# Print the RMSE values for each degree
print(rmse_values)
# Hierdie is wat ons gekry het van die backward stepwise model wat vir ons die beste results gee.
# Hierdie model sluit net die variables in en nie die interaction terms nie.
bwd4First <- lm(price ~ (areaA + areaB + areaC + seasonWinter + seasonAutumn +
close_to_beachYes + seasonSpring + max_guests + rating +
size + bathrooms + dist)^2, train)
#summary(bwd4)
# Ons gebruik hier die ^2 om al die one level interactions te vind,
# gebruik dan die summary function om al die significant interactions te vind.
# Significant interactions
#- areaSouthern suburbs:seasonSpring, areaBloubergstrand region:guestsToBath, areaBloubergstrand region:seasonAutumn
#- size:areaCity bowl, rating:seasonSpring, dist:guestsToBath .
# Hierdie is die beste model met interactions en predictor vars, die predictor var
# is elk tot n sekere degree ge"polynomial" wat dit maximize.
# Ek het probeer kyk watse waardes elke een van hulle die beste impact deur tedious
# trial en error en hierdie waardes is dit wat bo uitgekom het, bv 10,25,25,3,25.
pPoly_M_LM <- lm(price ~ areaA + areaB + areaC + seasonWinter+ seasonAutumn + close_to_beachYes
+ seasonSpring + poly(max_guests, 10) + poly(rating, 25) + poly(size, 25)
+ poly(bathrooms, 3) + poly(dist, 18) + areaC:seasonSpring +areaB:seasonSpring + areaB:seasonWinter
+ areaA:seasonSpring + (areaS * seasonAutumn) + areaS:seasonSummer
+ areaB:close_to_beachYes + areaC:close_to_beachYes, train)
#summary(pPoly_M_LM)
fLM <- pPoly_M_LM
# Haal hier al die vertical outliers uit, dit verbeter die data so bietjie
# omdat jy nie toets op data wat onalgemeen is nie.
# Die test data  vra nie dat jy extrapolate nie so vir my is dit beter om hierdie
# 257 vertical outliers uit te haal.
# Sit ook vir die pPoly_M_LM as die fLM(plekhouer vir final model met die tests)
no7_I <- pPoly_M_LM
std_resid <- rstandard(no7_I)
scaled_data <- datPP[abs(std_resid) <= 2, ]
# Original means and stdevs for descaling
dist_mean <- mean(datPP$dist)
dist_stdev <- sd(datPP$dist)
rating_mean <- mean(datPP$rating)
rating_stdev <- sd(datPP$rating)
size_mean <- mean(datPP$size)
size_stdev <- sd(datPP$size)
rooms_mean <- mean(datPP$rooms)
rooms_stdev <- sd(datPP$rooms)
guests_mean <- mean(datPP$max_guests)
guests_stdev <- sd(datPP$max_guests)
bath_mean <- mean(datPP$bathrooms)
bath_stdev <- sd(datPP$bathrooms)
descale_mean <- c(dist_mean, rating_mean, size_mean, rooms_mean, guests_mean, bath_mean)
descale_stdev <- c(dist_stdev, rating_stdev, size_stdev, rooms_stdev, guests_stdev, bath_stdev)
# Descale the numeric variables
numeric_cols_descale <- c("dist", "rating", "size", "rooms", "max_guests", "bathrooms")
for (i in seq_along(numeric_cols_descale)) {
scaled_data[[numeric_cols_descale[i]]] <- (scaled_data[[numeric_cols_descale[i]]] * descale_stdev[i]) + descale_mean[i]
}
descaled_data <- scaled_data
set.seed(1234)
sample <- sample(c(TRUE, FALSE), nrow(descaled_data), replace=TRUE, prob=c(0.8,0.2))
train_desc  <- descaled_data[sample, ]
valid_desc   <- descaled_data[!sample, ]
# Toets net hier die data teen die validation set voordat dit teen die test
# data getoets word.
# Die validation data en die training data kom van dieselfde datastel wat gesplit word
# so dit beteken al is hulle gestandardize behoort dit geen verskille te
# maak in die akkuraatheid van die skatting nie.
# Al wat mens oor moet seker maak is of die price var gestandardize is,
# en as dit is, moet jy dit terug transform.
predfLM <- fLM %>% predict(valid_desc)
predicted_prices_valid <- predict(fLM, newdata = valid_desc)
valid_desc$predicted_price <- predicted_prices_valid
validation_rmse <- rmse(predfLM, valid_desc$price)
# Hierdie is nogal belangrik, ons maak seker dat die test data op die selfde
# ge-"transform" is in terme van derivative vars wat ons gemaak het soos:
# areaA, areaB, areaC, areaS, seasonAutumn, seasonSpring ens....
testPP <- test
# Convert 'season' and 'close_to_beach' into factors if not already
testPP$season <- as.factor(testPP$season)
testPP$close_to_beach <- as.factor(testPP$close_to_beach)
# Create dummy variables for each season level
season_dummies <- model.matrix(~ season - 1, data = testPP)
# Add the dummy variables back to the original data frame
testPP <- cbind(testPP, season_dummies)
testPP$season <- NULL
# Convert 'close_to_beach' to a factor (if it's not already)
testPP$close_to_beach <- as.factor(testPP$close_to_beach)
# Create a dummy variable for 'close_to_beach'
# Assuming "Yes" = 1, "No" = 0
testPP$close_to_beachYes <- ifelse(testPP$close_to_beach == "Yes", 1, 0)
# Optionally, remove the original 'close_to_beach' column
testPP$close_to_beach <- NULL
# Create dummy variables for each level of 'area'
area_dummies <- model.matrix(~ area - 1, data = testPP)
# Rename the columns to the desired names
colnames(area_dummies) <- c("areaA", "areaB", "areaC", "areaS")
# Add the dummy variables back to the original data frame
testPP <- cbind(testPP, area_dummies)
area_num <- NULL
# Scale numerical variables (standardization)
numeric_cols <- c("dist", "rating", "size", "rooms", "max_guests", "bathrooms")
testPP[numeric_cols] <- scale(testPP[numeric_cols])
scaled_test_data <- testPP
predicted_prices_test <- predict(fLM, newdata = scaled_test_data)
scaled_test_data$predicted_price_test <- predicted_prices_test
#write.table(scaled_test_data$predicted_price_test, file = "Round3PredFINAL.txt", sep = "\t", row.names = FALSE, quote = TRUE)
predValues <- read.csv("Round3PredFINAL.txt", sep = ",")
PredTest <- cbind(predValues, test)
View(PredTest)
setwd("F:\\Stellenbosch Uni\\Semester2\\DSProject")
dat <- read.csv("F:\\Stellenbosch Uni\\Semester2\\DSProject\\project_airbnb_data.txt", sep = ";",
header = T, stringsAsFactors = T)
test <- read.csv("F:\\Stellenbosch Uni\\Semester2\\DSProject\\project_airbnb_data_test.txt", sep = ";",
header = T, stringsAsFactors = T)
library(leaps)
library(nortest)
library(dplyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(Metrics)
library(car)
# Ons install extra vir die Metrics en car package.
# Metrics is om die RMSE te calculate
# car is om die autocorrelation te calculate
datPP <- dat
# Convert 'season' and 'close_to_beach' into factors if not already
datPP$season <- as.factor(datPP$season)
datPP$close_to_beach <- as.factor(datPP$close_to_beach)
# Create dummy variables for each season level
season_dummies <- model.matrix(~ season - 1, data = datPP)
# Add the dummy variables back to the original data frame
datPP <- cbind(datPP, season_dummies)
datPP$season <- NULL
# Convert 'close_to_beach' to a factor (if it's not already)
datPP$close_to_beach <- as.factor(datPP$close_to_beach)
# Create a dummy variable for 'close_to_beach'
# Assuming "Yes" = 1, "No" = 0
datPP$close_to_beachYes <- ifelse(datPP$close_to_beach == "Yes", 1, 0)
# Optionally, remove the original 'close_to_beach' column
datPP$close_to_beach <- NULL
# Create dummy variables for each level of 'area'
area_dummies <- model.matrix(~ area - 1, data = datPP)
# Rename the columns to the desired names
colnames(area_dummies) <- c("areaA", "areaB", "areaC", "areaS")
# Add the dummy variables back to the original data frame
datPP <- cbind(datPP, area_dummies)
datPP$area_num <- NULL
# Scale numerical variables (standardization)
numeric_cols <- c("dist", "rating", "size", "rooms", "max_guests", "bathrooms")
datPP[numeric_cols] <- scale(datPP[numeric_cols])
# Calculate Z-scores for price and filter out extreme outliers horizontal
z_scores <- scale(datPP$price)
datPP <- datPP[abs(z_scores) < 3, ]  # Keep only data within 3 standard deviations
# Onthou om te se hoeveel observations remove is en hoeveel nuwe vars create is.
scaled_data <- datPP
set.seed(1234)
sample <- sample(c(TRUE, FALSE), nrow(scaled_data), replace=TRUE, prob=c(0.8,0.2))
train  <- scaled_data[sample, ]
valid   <- scaled_data[!sample, ]
baseModel <- lm(price ~ 1, train)
fwd <- step((baseModel), direction= "forward", list(lower=~1, upper=~dist+seasonAutumn+seasonSpring+                                                    rating+seasonSummer+areaA+areaB+areaC+areaS+bathrooms+close_to_beachYes+
seasonWinter+max_guests+size+rooms), data = train
, trace = F)
model_All <- lm(price ~ areaA + areaB + areaC + areaS + seasonWinter + seasonAutumn + close_to_beachYes +
seasonSpring + max_guests + rating + size + bathrooms + dist, train)
bwd <- step((model_All), direction= "backward", list(lower=~1, upper=~.),  data = train
, trace = F)
# Het hier vir die bwd model se vars gebruik omdat dit n laer AIC het.
evaluate_polynomial_model <- function(degree, train_data) {
model <- lm(price ~ areaA + areaB + areaC + seasonWinter + seasonAutumn
+ close_to_beachYes + seasonSpring + poly(max_guests, 10)
+ poly(rating, d) + poly(size, d)
+ poly(bathrooms, 3) + poly(dist, d), train)
predictions <- predict(model, newdata = train_data)
rmse <- sqrt(mean((predictions - train_data$price)^2))
return(rmse)
}
# Initialize a vector to store RMSE values
degree_range <- 1:10
rmse_values <- numeric(length(degree_range))
# Evaluate models for different polynomial degrees
for (d in degree_range) {
rmse_values[d] <- evaluate_polynomial_model(d, train)
}
# Print the RMSE values for each degree
print(rmse_values)
# Hierdie is wat ons gekry het van die backward stepwise model wat vir ons die beste results gee.
# Hierdie model sluit net die variables in en nie die interaction terms nie.
bwd4First <- lm(price ~ (areaA + areaB + areaC + seasonWinter + seasonAutumn +
close_to_beachYes + seasonSpring + max_guests + rating +
size + bathrooms + dist)^2, train)
#summary(bwd4)
# Ons gebruik hier die ^2 om al die one level interactions te vind,
# gebruik dan die summary function om al die significant interactions te vind.
# Significant interactions
#- areaSouthern suburbs:seasonSpring, areaBloubergstrand region:guestsToBath, areaBloubergstrand region:seasonAutumn
#- size:areaCity bowl, rating:seasonSpring, dist:guestsToBath .
# Hierdie is die beste model met interactions en predictor vars, die predictor var
# is elk tot n sekere degree ge"polynomial" wat dit maximize.
# Ek het probeer kyk watse waardes elke een van hulle die beste impact deur tedious
# trial en error en hierdie waardes is dit wat bo uitgekom het, bv 10,25,25,3,25.
pPoly_M_LM <- lm(price ~ areaA + areaB + areaC + seasonWinter+ seasonAutumn + close_to_beachYes
+ seasonSpring + poly(max_guests, 10) + poly(rating, 25) + poly(size, 25)
+ poly(bathrooms, 3) + poly(dist, 18) + areaC:seasonSpring +areaB:seasonSpring + areaB:seasonWinter
+ areaA:seasonSpring + (areaS * seasonAutumn) + areaS:seasonSummer
+ areaB:close_to_beachYes + areaC:close_to_beachYes, train)
#summary(pPoly_M_LM)
fLM <- pPoly_M_LM
# Haal hier al die vertical outliers uit, dit verbeter die data so bietjie
# omdat jy nie toets op data wat onalgemeen is nie.
# Die test data  vra nie dat jy extrapolate nie so vir my is dit beter om hierdie
# 257 vertical outliers uit te haal.
# Sit ook vir die pPoly_M_LM as die fLM(plekhouer vir final model met die tests)
no7_I <- pPoly_M_LM
std_resid <- rstandard(no7_I)
scaled_data <- datPP[abs(std_resid) <= 2, ]
# Original means and stdevs for descaling
dist_mean <- mean(datPP$dist)
dist_stdev <- sd(datPP$dist)
rating_mean <- mean(datPP$rating)
rating_stdev <- sd(datPP$rating)
size_mean <- mean(datPP$size)
size_stdev <- sd(datPP$size)
rooms_mean <- mean(datPP$rooms)
rooms_stdev <- sd(datPP$rooms)
guests_mean <- mean(datPP$max_guests)
guests_stdev <- sd(datPP$max_guests)
bath_mean <- mean(datPP$bathrooms)
bath_stdev <- sd(datPP$bathrooms)
descale_mean <- c(dist_mean, rating_mean, size_mean, rooms_mean, guests_mean, bath_mean)
descale_stdev <- c(dist_stdev, rating_stdev, size_stdev, rooms_stdev, guests_stdev, bath_stdev)
# Descale the numeric variables
numeric_cols_descale <- c("dist", "rating", "size", "rooms", "max_guests", "bathrooms")
for (i in seq_along(numeric_cols_descale)) {
scaled_data[[numeric_cols_descale[i]]] <- (scaled_data[[numeric_cols_descale[i]]] * descale_stdev[i]) + descale_mean[i]
}
descaled_data <- scaled_data
set.seed(1234)
sample <- sample(c(TRUE, FALSE), nrow(descaled_data), replace=TRUE, prob=c(0.8,0.2))
train_desc  <- descaled_data[sample, ]
valid_desc   <- descaled_data[!sample, ]
# Toets net hier die data teen die validation set voordat dit teen die test
# data getoets word.
# Die validation data en die training data kom van dieselfde datastel wat gesplit word
# so dit beteken al is hulle gestandardize behoort dit geen verskille te
# maak in die akkuraatheid van die skatting nie.
# Al wat mens oor moet seker maak is of die price var gestandardize is,
# en as dit is, moet jy dit terug transform.
predfLM <- fLM %>% predict(valid_desc)
predicted_prices_valid <- predict(fLM, newdata = valid_desc)
valid_desc$predicted_price <- predicted_prices_valid
validation_rmse <- rmse(predfLM, valid_desc$price)
# Hierdie is nogal belangrik, ons maak seker dat die test data op die selfde
# ge-"transform" is in terme van derivative vars wat ons gemaak het soos:
# areaA, areaB, areaC, areaS, seasonAutumn, seasonSpring ens....
testPP <- test
# Convert 'season' and 'close_to_beach' into factors if not already
testPP$season <- as.factor(testPP$season)
testPP$close_to_beach <- as.factor(testPP$close_to_beach)
# Create dummy variables for each season level
season_dummies <- model.matrix(~ season - 1, data = testPP)
# Add the dummy variables back to the original data frame
testPP <- cbind(testPP, season_dummies)
testPP$season <- NULL
# Convert 'close_to_beach' to a factor (if it's not already)
testPP$close_to_beach <- as.factor(testPP$close_to_beach)
# Create a dummy variable for 'close_to_beach'
# Assuming "Yes" = 1, "No" = 0
testPP$close_to_beachYes <- ifelse(testPP$close_to_beach == "Yes", 1, 0)
# Optionally, remove the original 'close_to_beach' column
testPP$close_to_beach <- NULL
# Create dummy variables for each level of 'area'
area_dummies <- model.matrix(~ area - 1, data = testPP)
# Rename the columns to the desired names
colnames(area_dummies) <- c("areaA", "areaB", "areaC", "areaS")
# Add the dummy variables back to the original data frame
testPP <- cbind(testPP, area_dummies)
area_num <- NULL
# Scale numerical variables (standardization)
numeric_cols <- c("dist", "rating", "size", "rooms", "max_guests", "bathrooms")
testPP[numeric_cols] <- scale(testPP[numeric_cols])
scaled_test_data <- testPP
predicted_prices_test <- predict(fLM, newdata = scaled_test_data)
scaled_test_data$predicted_price_test <- predicted_prices_test
write.table(scaled_test_data$predicted_price_test, file = "Round3PredFINAL.txt", sep = "\t", row.names = FALSE, quote = TRUE)
predValues <- read.csv("Round3PredFINAL.txt", sep = ",")
PredTest <- cbind(predValues, test)
View(PredTest)
install.packages("tinytex")
library(tinytex)
library(leaps)
library(nortest)
library(dplyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(Metrics)
library(car)
library(gridExtra)
knitr::opts_chunk$set(echo = TRUE)
?ggpairs
setwd("F:\\Stellenbosch Uni\\Semester2\\DSProject")
dat <- read.csv("F:\\Stellenbosch Uni\\Semester2\\DSProject\\project_airbnb_data.txt", sep = ";", header = T, stringsAsFactors = T)
test <- read.csv("F:\\Stellenbosch Uni\\Semester2\\DSProject\\project_airbnb_data_test.txt", sep = ";", header = T, stringsAsFactors = T)
install.packages("tinytex")
library(tinytex)
library(leaps)
library(nortest)
library(dplyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(Metrics)
library(car)
library(gridExtra)
knitr::opts_chunk$set(echo = TRUE)
install.packages("tinytex")
library(tinytex)
library(leaps)
library(nortest)
library(dplyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(Metrics)
library(car)
library(gridExtra)
knitr::opts_chunk$set(echo = TRUE)
library(tinytex)
library(leaps)
library(nortest)
library(dplyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(Metrics)
library(car)
library(gridExtra)
knitr::opts_chunk$set(echo = TRUE)
?ggpairs()
library(tinytex)
library(leaps)
library(nortest)
library(dplyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(Metrics)
library(car)
library(gridExtra)
knitr::opts_chunk$set(echo = TRUE)
ggpairs()
pairs(~ price + dist + rating + size + max_guests + bathrooms, data = scaled_data,
main = "Scatterplot Matrix")
pairs(~ price + dist + rating + size + max_guests + bathrooms, data = scaled_data,
+       main = "Scatterplot Matrix")
pairs(~ price + dist + rating + size + max_guests + bathrooms, data = scaled_data,
,main = "Scatterplot Matrix")
ggplot(scaled_data, aes(x = price)) +
geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
facet_grid(area ~ season) +
labs(title = "Price Distribution by Area and Season", x = "Price", y = "Count") +
 theme_minimal()
ggplot(scaled_data, aes(x = price)) +
geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
facet_grid(area ~ season) +
labs(title = "Price Distribution by Area and Season", x = "Price", y = "Count") +
theme_minimal()
ggplot(scaled_data, aes(x = price)) +
geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
facet_grid(area ~ season) +
labs(title = "Price Distribution by Area and Season", x = "Price", y = "Count") +
theme_minimal()
ggplot(scaled_data, aes(x = Price)) +
geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
facet_grid(area ~ season) +
labs(title = "Price Distribution by Area and Season", x = "Price", y = "Count") +
theme_minimal()
library(ggplot2)
library(ggpubr)
ggplot(scaled_data, aes(x = price)) +
geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
facet_grid(area ~ season) +
labs(title = "Price Distribution by Area and Season", x = "Price", y = "Count") +
theme_minimal()
scaled_data
ggplot(dat, aes(x = price)) +
geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
facet_grid(area ~ season) +
labs(title = "Price Distribution by Area and Season", x = "Price", y = "Count") +
theme_minimal()
# Get residuals and fitted values
residuals <- resid(pPoly_M_LM)
fitted_values <- fitted(pPoly_M_LM)
# Get residuals and fitted values
residuals <- resid(fLM)
fitted_values <- fitted(fLM)
ggplot(residuals_df, aes(x = Fitted, y = Residuals)) +
geom_point() +
geom_hline(yintercept = 0, color = "red") +  # Add horizontal line at y = 0
labs(title = "Residuals vs Fitted Values",
x = "Fitted Values",
y = "Residuals") +
theme_minimal()
# Get residuals and fitted values
residuals <- resid(fLM)
fitted_values <- fitted(fLM)
residuals_df <- data.frame(Fitted = fitted_values, Residuals = residuals)
ggplot(residuals_df, aes(x = Fitted, y = Residuals)) +
geom_point() +
geom_hline(yintercept = 0, color = "red") +  # Add horizontal line at y = 0
labs(title = "Residuals vs Fitted Values",
x = "Fitted Values",
y = "Residuals") +
theme_minimal()
residuals_df
